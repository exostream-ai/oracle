How does R work as a published benchmark?
This is the right question to ask and it exposes a real design tension. R is a function of the user's task profile — it takes n_in, n_out, n_think, η as inputs. Those are user-specific variables. You can't publish R as a ticker because R for my workload is different from R for yours.
What you can publish is β, and β is a clean ticker — it's a single observable number per model, per mode. The structural Greeks (r_in, r_cache, etc.) and the extrinsic parameters (θ, σ) are also publishable because they don't depend on any user input.
R is what happens when a user applies the oracle's published parameters to their own task profile. It's not a benchmark — it's a calculator result. The benchmark is β. The forward curve is on β. The index would be a weighted composite of β values. Tradable instruments settle against β.
So the architecture is actually clean: the oracle publishes model-level parameters (β, Greeks, θ, σ, forward prices). R is computed client-side — in the SDK, in the calculator, in the cost router — by combining the oracle's parameters with the user's task profile. The oracle never needs to know what anyone's workload looks like.
For the Inference Cost Index (3.1), you'd construct it as a weighted basket of β values, not R values. Something like: take the top 10 models by estimated market usage, weight them by some proxy of adoption (API tier popularity, benchmark participation, developer survey data), and compute a weighted average β. That's your index. It moves when providers reprice, when the weights shift due to adoption changes, or when new models enter/exit the basket. No user-specific assumptions needed.
The one place you do need standardized assumptions is in marketing materials and comparison tools. When the Model Comparator shows "Opus 4.5 costs $0.162/query vs GPT-4.1 costs $0.029/query," those numbers assume a reference task profile. You'd publish that reference profile transparently — something like "standard RAG workload: 10K input, 500 output, 40% cache, sync" — and let users plug in their own numbers for personalized results. This is exactly what commodity benchmarks do: WTI crude is priced for delivery at Cushing, Oklahoma — a specific standardized condition that everyone knows isn't their actual delivery point, but serves as the reference.

The components of R require: n_in, n_out, n_think, η, and the model. If you're sitting in the request path — as middleware, an MCP tool, or an observability integration — you can observe or estimate all of these.
n_in is knowable before the request is sent. You tokenize the prompt and system message. Every model family has a public tokenizer — Anthropic publishes theirs, OpenAI has tiktoken, Google has their tokenizer. The token count is deterministic. You know n_in exactly.
n_out is the problem. You don't know it until the response is complete. But you can estimate it in a few ways. If the user specifies max_tokens in the API call, that's an upper bound. If you have historical data on the user's typical output lengths for similar prompts (similar system message, similar task type), you can predict it. For a cost estimate before the call, a reasonable default or a user-provided expected output length works. For a cost meter after the call, you have the exact number from the response metadata.
n_think is only relevant for reasoning models and it's the hardest to predict upfront. Thinking token usage varies wildly based on problem complexity. You basically can't estimate it before the call. After the call, the API response includes thinking token counts for models that support extended thinking. So pre-call estimates for reasoning models will be noisy, but post-call metering is exact.
η is tricky in a different way. The cache hit ratio isn't a property of a single prompt — it's a property of the user's session pattern. If they're sending the same system prompt and context prefix repeatedly (typical in RAG pipelines), η is high. If every request is unique context, η is zero. You can't determine η from a single prompt. But if you're observing a stream of requests, you can compute η empirically by tracking which prompt prefixes repeat. Alternatively, the provider's response metadata sometimes indicates whether caching was applied, which gives you the ground truth after the fact.
The model is known — it's in the API call.
So the realistic picture is: pre-call, you can compute a cost estimate using exact n_in, estimated n_out, and assumed η. Post-call, you can compute the exact cost using all observed values. The pre/post gap is mostly about output length uncertainty and thinking tokens.

What this enables practically.
The most valuable implementation is a cost metering layer that wraps the user's LLM API calls. It observes the request, tokenizes the input, watches the response, and then calls the Exostream task pricer with the exact observed values. The user gets a real-time cost log: every API call annotated with its actual cost, κ, and how much cache or compression would have saved.
This is the observability integration play (product 2.3) made concrete. The Exostream SDK could expose a wrapper:
pythonimport exostream

client = exostream.wrap(anthropic.Client())
response = client.messages.create(
    model="claude-sonnet-4-5-20250514",
    max_tokens=1000,
    messages=[{"role": "user", "content": prompt}]
)

# response.exostream is automatically populated
print(response.exostream.cost)       # $0.0043
print(response.exostream.kappa)      # 2.31
print(response.exostream.cache_savings)  # $0.0018 if η were 0.6
The wrapper tokenizes the input before sending, captures the output token count from the response, calls the oracle, and attaches the cost metadata. Zero friction for the developer — they swap in the wrapped client and get cost intelligence on every call.
For the pre-call estimate use case — "how much will this prompt cost before I send it?" — the SDK could offer:
pythonestimate = exostream.estimate(
    model="claude-sonnet-4-5-20250514",
    prompt=my_prompt,
    expected_output=500  # user's estimate or default
)
print(estimate.cost_range)  # $0.003 - $0.008
This gives a range rather than a point estimate, reflecting the output length uncertainty. The range bounds come from assuming minimum and maximum realistic output lengths for the given max_tokens setting.

The agent use case is where this gets really powerful. An AI agent making autonomous LLM calls has a budget constraint. It needs to know "can I afford to make this call?" before making it. The pre-call estimator gives it that. And the post-call meter lets it track cumulative spend against its budget in real time. The agent doesn't need to understand the pricing model — it just calls the Exostream API with its prompt and gets back a cost.
Combined with x402 this becomes a complete economic layer for autonomous agents: the agent checks its budget, estimates the cost of the next inference call via Exostream, decides whether to proceed, makes the call, meters the actual cost, and updates its remaining budget. All programmatic, all driven by the oracle's canonical pricing.
That's not V1 — but the API design should accommodate it from the start. The task pricer endpoint accepting raw prompt text (which it tokenizes server-side) alongside or instead of explicit token counts would make this seamless. Adds a small amount of compute per request (tokenization) but nothing the server can't handle.

To get n_in from a prompt, you have to tokenize it. Tokenization is compute. The question is how much.
Tokenization is not inference. It's a deterministic string-splitting operation using a fixed vocabulary lookup — essentially a dictionary scan across the text. There's no neural network involved, no GPU required. It runs on CPU in milliseconds. Tokenizing a 30K token prompt takes maybe 5-10ms on a cheap VPS. It's comparable to running a regex over the text. The compute cost is effectively zero at any reasonable scale.
Every major model family has a public tokenizer you can run locally. Tiktoken for OpenAI models is a Python/Rust library. Anthropic's tokenizer is available. Google's SentencePiece tokenizers are open source. You'd bundle these in the SDK and tokenization happens client-side — it never even hits your server.
So the answer is: yes, it requires compute, but the compute is so trivial that it's not worth metering or worrying about. The SDK tokenizes the prompt locally on the user's machine in single-digit milliseconds, gets the exact n_in, and sends that integer to the Exostream API. No prompt text leaves the user's environment, no heavy computation happens anywhere.
The only real cost is bundling multiple tokenizers in the SDK. Each tokenizer's vocabulary file is a few megabytes. Supporting all 6-7 provider families means shipping maybe 20-30MB of tokenizer assets. That's fine for a Python package, slightly heavy for a browser-based tool. For the web calculator, you'd either use a lightweight JavaScript tokenizer (tiktoken has a WASM port) or just let the user input token counts directly with a "don't know? paste your prompt here" option that tokenizes in the browser.

On the schema — a few design notes. The v_greek_sheet view is the money query: one row per active model, all parameters resolved (with model-level overrides falling through to family defaults via COALESCE). That single view is basically the oracle's entire publishable state. Context tiers are temporal (effective_from/effective_to) so when a provider introduces tiered pricing, you don't lose the historical flat-pricing record. And page_snapshots stores raw HTML — you'll want this for audit trail when someone disputes a price point.

data sources, a few things worth flagging:
The Meta problem is real. Every other provider has a canonical origin price you can scrape. Meta doesn't — open-weight models are hosted everywhere at different prices. I'd recommend median across top 3 hosts (Together, Fireworks, Groq) as a composite reference, clearly labeled. It's imperfect but it's what commodity markets do when there's no single exchange — they construct a reference from dealer quotes.

Google is your canary for tiered pricing. They already price some Gemini models differently above and below 128K context. That's the τ/α system live in the wild. When you build the scraper, Google's the one where you need to parse tier breakpoints from day one.
The hardest operational problem isn't getting the data — it's detecting changes fast. Provider pricing pages don't have RSS feeds or webhooks. You're polling HTML and diffing. The blog/announcement channels give you early warning but they're unstructured. The real competitive advantage of the oracle is latency between a price change happening and your feed reflecting it.

θ bootstrap is feasible but thin for newer providers. Anthropic and OpenAI have enough price history to fit meaningful decay curves. xAI, DeepSeek, and Mistral have maybe 1-2 data points each. Family priors will do heavy lifting for the first few months.

Getting pricing data directly from using the models.
This is a clever idea and it has real advantages over scraping, but also some complications. Let me think through it.
The concept: instead of scraping pricing pages, you make actual API calls to each provider and observe the cost reported in the response. Most LLM API responses include usage metadata — token counts and sometimes cost — in the response headers or body. You'd send a minimal standardized prompt to each model periodically and record the actual billed cost.
Advantages over scraping. First, it's authoritative. You're not parsing HTML that might be cached, stale, or differently formatted — you're getting the actual price the provider is charging right now for real usage. Second, it's structurally stable. API response formats change less frequently than pricing page HTML. Third, it detects hidden pricing changes that might not be reflected on the pricing page immediately — providers sometimes update their billing systems before updating their marketing pages. Fourth, it gives you real cache pricing data. You can send the same prompt twice and observe the cache discount directly, verifying r_cache empirically rather than trusting the published number.
How you'd implement it. Define a canonical test prompt — something small, deterministic, with a fixed output length. Send it to every tracked model once per hour (or whatever cadence you want). Record the input tokens, output tokens, and billed cost from the response metadata. Compute the implied β from the response: β_observed = (billed_cost / output_tokens) × 10⁶. Compare against the last known β. Alert on any deviation.
For cache testing specifically: send the same prompt twice in quick succession. First call gives you the uncached price, second call (if the provider supports prompt caching) gives you the cached price. The ratio between them is your empirically verified r_cache.
The cost of this approach. Each test call is tiny — maybe 100 input tokens, 50 output tokens. At Opus 4.5 pricing that's about $0.003 per call. Across 17 models, once per hour, 24 hours a day: 17 × 24 × $0.003 ≈ $1.22/day ≈ $37/month. Not nothing, but manageable. And for cheaper models (Haiku, GPT-4.1 mini, Flash) the per-call cost is negligible.
The real cost is maintaining API keys and billing accounts with every provider. You need active accounts with Anthropic, OpenAI, Google, xAI, Mistral, DeepSeek, and whatever Meta hosting provider you use as the reference. That's 7+ accounts, each with their own billing, API key rotation, rate limits, and terms of service.
Complications. Some providers don't return cost in the API response — they only show usage (token counts) and you have to compute cost yourself from the published rate. So you still need the published rate as a reference, which means you can't fully eliminate scraping. Also, some providers have different pricing for different API tiers, regions, or commitment levels. Your test call might hit a different pricing tier than what's published on the marketing page. You'd need to ensure your test account is on the standard pay-as-you-go tier.
Terms of service risk. Some providers might not love automated pricing surveillance via their API. It's not clearly prohibited by most ToS (you're making legitimate API calls and paying for them), but it's a grey area. Scraping public pricing pages has stronger legal standing (publicly available information) than programmatic price discovery through API usage.
My recommendation: use both. Scraping is your primary data source because it's cheap, it's legally clean, and it gives you the published reference rate. API-based price verification is your secondary source — a cross-check that catches discrepancies between published and actual pricing. When the two disagree, that's a signal worth investigating and potentially alerting users about. The combination is stronger than either alone, and the API-based verification adds credibility: "we don't just read the pricing page, we verify it against live billing data."
One more angle: if you're already making API calls for verification, you can also collect latency data (time-to-first-token, total response time) as a byproduct. That's not in the current model but it's valuable context — it could feed a future product around inference performance benchmarking, or inform the batch vs sync pricing dynamics.

At launch you have zero oracle-collected data. No time series, no θ computed from your own observations, no σ. The dashboard would show spot prices (scrapeable right now) and then a bunch of empty charts. That's not a good look for something positioning itself as the Bloomberg of inference pricing.
But you actually have more than zero. You have the reconstructible public record, and how you handle that distinction is both a data integrity question and a brand-building opportunity.

What's recoverable right now.
Provider pricing changes are announced publicly — blog posts, changelogs, X posts, documentation updates. Most major pricing pages have been archived by the Wayback Machine multiple times. Between these sources you can reconstruct a sparse but real price history for the major models going back to GPT-4's launch in March 2023. That's roughly two and a half years of data points.
It's sparse — you're talking maybe 5-15 price points per model family, not daily observations. But for θ estimation that's actually enough. You're fitting an exponential decay to a handful of known points, and the signal is so strong (GPT-4 falling from $60 to $8 over two years) that even sparse data gives you a meaningful decay rate.
What you'd do concretely: before launch, spend a day or two pulling every recoverable price point. Wayback Machine snapshots of the pricing pages, provider blog posts announcing changes, archived documentation. Structure it all into the spot_prices table with source = 'historical:wayback' or source = 'historical:blog' and the actual date of observation. Be meticulous about timestamps — you want the date the price took effect, not the date you found it.
For the Claude family, Anthropic's blog has dated announcements for every major release and pricing change. OpenAI similarly. Google's Vertex AI pricing has been archived. The newer providers (xAI, DeepSeek) have shorter histories but their pricing hasn't changed much, so there's less to reconstruct.

The API publishes the oracle's parameters. R is computed client-side from those parameters plus the user's task profile. The API never needs to know anyone's workload. But you're right that just publishing raw parameters and expecting every user to implement the math themselves creates friction. You need a computation endpoint alongside the data endpoints.
So the API has two modes.
Mode 1: Data feed. Returns raw oracle parameters. The user gets β, Greeks, θ, σ, forward curves — the full Greek sheet. This is what observability platforms, routers, and power users consume. They plug the parameters into their own cost accounting logic. This is the feed you cache aggressively and serve at scale.
Mode 2: Task pricer. The user sends a task profile and gets back a fully computed cost. This is the calculator as an API endpoint. The user posts their n_in, n_out, n_think, η, model choice, and optionally a forward horizon t. The API returns S, κ, the effective input rate, forward cost, and the Δ values. The computation is trivial — it's the same arithmetic from the stress test, a few microseconds per request. No database hit required beyond reading the cached oracle state.
The endpoint would look something like:
POST /v1/price
{
  "model": "opus-4.5",
  "mode": "sync",
  "n_in": 30000,
  "n_out": 800,
  "n_think": 0,
  "eta": 0.6,
  "horizon_months": 3
}
And returns:
{
  "spot": {
    "cost_usd": 0.1618,
    "kappa": 4.49,
    "r_in_eff": 0.0932,
    "beta_used": 45.00
  },
  "forward": {
    "cost_usd": 0.1475,
    "beta_forward": 41.00,
    "theta_used": 0.031,
    "decay_factor": 0.911,
    "horizon_months": 3
  },
  "deltas": {
    "cache_value": 0.144,
    "price_sensitivity": 0.003596
  },
  "oracle_timestamp": "2026-02-04T14:30:00Z"
}
This is the endpoint the dashboard and calculator both call. It's also what the MCP server wraps. And it's what any developer integrates when they want cost-aware logic in their application without implementing the math.
You'd also want a batch comparison variant — same task profile, all models — so the user can see ranked costs across the entire market in one call:
POST /v1/price/compare
{
  "n_in": 30000,
  "n_out": 800,
  "eta": 0.6
}
Returns an array of every active model with spot cost, κ, and forward cost, sorted cheapest first. This is what powers the model comparator on the dashboard and it's what a cost-aware router would call to make routing decisions.
The critical design point: the task pricer endpoint is stateless and user-agnostic. It doesn't store the task profile, it doesn't track who's asking, it doesn't build user-specific models. It just applies the published parameters to whatever inputs it receives. This keeps the oracle clean — it's infrastructure, not a SaaS that knows your business. The user's R is computed fresh on every call from the current oracle state.
For users who want persistent tracking — "show me how my costs have evolved over the past month" — that's application-layer logic built on top of the API, not something the oracle itself provides. The dashboard could do this client-side with local storage, or a future premium tier could offer historical cost tracking where the user opts in to storing their task profile. But the core API stays stateless.
The MCP server maps directly to these two endpoints. One tool for "get me the current Greek sheet for Opus 4.5" (data feed), one tool for "what would it cost to run 30K context, 800 output on Opus 4.5 with 60% cache?" (task pricer). An agent working in Claude or Cursor can call either depending on whether it needs raw parameters or a computed answer.

Price Per Token and similar sites are themselves scraping provider pricing pages — they're aggregators of public data, not primary sources. You'd be scraping a scraper. The data you actually need lives on the provider pricing pages directly, which are public, intended for consumption, and the authoritative source. There's no reason to add a dependency on a third-party aggregator when you can go to the origin.
The one thing Price Per Token has that might be tempting is their coverage breadth — 304 models including lots of small open-source providers you might not want to track individually at launch. But Exostream doesn't need 304 models. The oracle's value is depth on the models that matter, not breadth across everything. 17 tickers with full Greeks, forward curves, and historical data is worth more than 300 flat prices.
If you later want to expand coverage to long-tail models, build your own scrapers for the providers that matter. It's cleaner legally, more reliable technically, and maintains your independence as a primary data source.

On output token explosion and the honesty problem.
You're identifying something real and it's worth being precise about what the oracle can and can't do here.
The stress test validated the math assuming known token counts. Every test case said "given n_in=30000 and n_out=800, the cost is X." The math is correct. But when a user pastes a prompt into the calculator and asks "what will this cost?", the oracle knows n_in exactly (tokenization) and knows absolutely nothing about n_out.
And n_out doesn't just vary a little. It can vary by orders of magnitude based on prompt content, system instructions, and model behavior. A prompt that says "answer yes or no" might produce 3 tokens. A prompt that says "write a detailed analysis" might produce 4000. A reasoning model asked to solve a hard math problem might burn 50,000 thinking tokens before producing 200 output tokens. The same model given an easy question might think for 500 tokens. The relationship between prompt content and output length is essentially the full complexity of the model's behavior — predicting it accurately is running the model.
So what do you show the user? A few options, and I think you need a combination.
The honest range. When a user submits a prompt without specifying expected output, show them a cost range, not a point estimate. Anchor the range to something meaningful: the minimum is the cost assuming n_out equals 1 (just the input cost), the maximum is the cost assuming n_out equals max_tokens from their API configuration (or a sensible default like 4096). For reasoning models, the range gets much wider because n_think is unbounded up to the model's thinking budget.
Display it as a bar or gradient: "This prompt will cost between $0.02 and $0.31 depending on response length." That's honest and useful. The user immediately sees that the uncertainty is almost entirely on the output side.
κ at the extremes. Show κ computed at both ends of the range. At n_out=1, κ is enormous — almost all cost is input. At n_out=4096, κ drops toward 1 because output dominates. This is actually a powerful visualization: the user sees that for short responses, their context depth is the cost driver, and for long responses, the output price dominates. That's a genuinely useful insight for workload design.
Post-hoc actuals. For the SDK wrapper that observes real API calls, you report exact cost after the fact. No estimation needed. The response comes back, you count the tokens, you price it. Over time, the user accumulates a distribution of their actual n_out for different prompt types, and the estimate range tightens based on their own usage patterns. But that's application-layer logic, not the oracle.
What you should NOT do. Don't try to predict n_out from prompt content. Don't build a model that says "this looks like a coding prompt, so expect 1500 output tokens." That's a prediction about model behavior, not a pricing function, and it'll be wrong often enough to undermine trust in the oracle. The oracle prices tokens. How many tokens a prompt generates is the user's problem, or something a separate product could tackle.
The dashboard should make this uncertainty visible rather than hiding it. A slider that lets the user adjust expected n_out and watch the cost update in real time is more useful and more honest than a single point estimate. Default the slider to some reasonable midpoint, let them drag it, and show how cost responds. That interactivity communicates the sensitivity better than any number could.
For the API endpoint, accept n_out as a required parameter and make the caller responsible for their own estimate. If they don't provide one, return results at multiple output levels — say 100, 500, 1000, 4000 — so the consumer can interpolate. Don't default to a single guess that looks authoritative but isn't.
The key insight: the oracle's value isn't predicting total cost from a prompt. It's pricing the tokens accurately and making the cost structure transparent. The user learns that their 30K context RAG pipeline has most of its cost exposure in input tokens, that cache would save them 40%, that prices are falling at 3% per month. Those insights are valuable regardless of whether any individual call generates 200 or 2000 output tokens. The per-call cost is noisy. The structural economics are stable. Lead with the structure.